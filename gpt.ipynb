{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21397,"status":"ok","timestamp":1710563832309,"user":{"displayName":"He Nan Li (Tony)","userId":"01291724548798117557"},"user_tz":420},"id":"JSZ5i4yG7-lp","outputId":"4d011ad4-f066-48b2-94f0-ad3237bcf73f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting datasets\n","  Downloading datasets-2.18.0-py3-none-any.whl (510 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n","Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n","Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n","Collecting xxhash (from datasets)\n","  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting multiprocess (from datasets)\n","  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\n","Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.3)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.10.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n","Installing collected packages: xxhash, dill, multiprocess, datasets\n","Successfully installed datasets-2.18.0 dill-0.3.8 multiprocess-0.70.16 xxhash-3.4.1\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.2)\n"]}],"source":["# @title init for running on colab\n","!pip install datasets\n","!pip install tqdm\n","\n","import torch\n","import torch.nn as nn\n","from torch.nn import functional as F\n","from transformers import AutoTokenizer\n","from datasets import load_dataset\n","from tqdm import tqdm\n"]},{"cell_type":"code","execution_count":2,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1710563832309,"user":{"displayName":"He Nan Li (Tony)","userId":"01291724548798117557"},"user_tz":420},"id":"CCarHxmF6Kxt","outputId":"12a5235b-8d98-4a3e-aa9d-10fc707be923"},"outputs":[{"output_type":"stream","name":"stdout","text":["DATASET_SOURCE='local_file'\n","final input text length 1115394\n"]}],"source":["# @title load data source\n","\n","\n","DATASET_SOURCE = 'local_file' # @param [\"wikimedia/wikipedia/20231101.zh-classical\", \"local_file\"]\n","\n","LOCAL_FILE = 'input.txt' # @param {type:\"string\"}\n","\n","print(f'{DATASET_SOURCE=}')\n","\n","def load_dataset_from_source():\n","    if DATASET_SOURCE == 'local_file':\n","        with open(LOCAL_FILE, 'r', encoding='utf-8') as f:\n","            text = f.read()\n","        print('final input text length', len(text))\n","        return text\n","\n","    text = ''\n","    if DATASET_SOURCE == 'wikimedia/wikipedia/20231101.zh-classical':\n","        dataset = load_dataset('wikimedia/wikipedia', '20231101.zh-classical')\n","        num_rows = dataset['train'].num_rows\n","\n","    for i in range(num_rows):\n","        text += dataset['train'][i]['text']\n","    print('final input text length', len(text))\n","    return text\n","\n","\n","INPUT_TEXT = load_dataset_from_source()"]},{"cell_type":"code","execution_count":3,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23718,"status":"ok","timestamp":1710563857714,"user":{"displayName":"He Nan Li (Tony)","userId":"01291724548798117557"},"user_tz":420},"id":"b7khzRaHhx0p","outputId":"e972557f-a106-4ea1-e5e8-3d677ebd30c8"},"outputs":[{"output_type":"stream","name":"stdout","text":["TOKENIZATION_ALG='bpe' TRAINING_DATA_SPLIT=0.9\n","begin byte pair tokenization\n","merging (101, 32) into a new token 256\n","merging (116, 104) into a new token 257\n","merging (116, 32) into a new token 258\n","merging (115, 32) into a new token 259\n","merging (100, 32) into a new token 260\n","merging (44, 32) into a new token 261\n","merging (111, 117) into a new token 262\n","merging (101, 114) into a new token 263\n","merging (105, 110) into a new token 264\n","merging (121, 32) into a new token 265\n","merging (97, 110) into a new token 266\n","merging (58, 10) into a new token 267\n","merging (111, 114) into a new token 268\n","merging (111, 32) into a new token 269\n","merging (101, 110) into a new token 270\n","merging (10, 10) into a new token 271\n","merging (97, 114) into a new token 272\n","merging (32, 257) into a new token 273\n","merging (111, 110) into a new token 274\n","merging (108, 108) into a new token 275\n","original token length 1115394; now token length 882737; compression %: 1.26X\n","vocab_size=276\n"]}],"source":["# @title select tokenization alg\n","\n","TOKENIZATION_ALG = 'bpe'  # @param [\"default\", \"tiny-llama-fast-tokenizer\", \"bpe\"]\n","TRAINING_DATA_SPLIT = 0.9  # @param {type:\"slider\", min:0, max:1, step:0.05}\n","\n","# @markdown The following hyper parameters are only applicable for BPE\n","BPE_VOCAB_SIZE = 276 # @param {type:\"number\"}\n","\n","print(f'{TOKENIZATION_ALG=} {TRAINING_DATA_SPLIT=}')\n","\n","\n","def bpe():\n","\n","    def get_byte_pair_stats(tokens):\n","        \"\"\"\n","        Return one byte pair with the highest frequency\n","        \"\"\"\n","        counts = {}\n","        for pair in zip(tokens, tokens[1:]): # iterate through consecutive elements\n","            counts[pair] = counts.get(pair, 0) + 1\n","        return counts\n","\n","\n","    def merge_byte_pair(tokens, pair, new_token):\n","        \"\"\"\n","        tokens: list of token bytes\n","        pair: byte pair to be replaced\n","        new_token: new byte\n","\n","        return a new list of token bytes after replacement\n","\n","        example: merge_byte_pair([5,6,6,7,9], (6,7), 99) -> [5, 6, 99, 9]\n","        \"\"\"\n","        i = 0\n","        new_tokens = []\n","        while i < len(tokens):\n","            if i < len(tokens) - 1 and tokens[i] == pair[0] and tokens[i+1] == pair[1]:\n","                new_tokens.append(new_token)\n","                i += 2\n","            else:\n","                new_tokens.append(tokens[i])\n","                i += 1\n","        return new_tokens\n","\n","    print(\"begin byte pair tokenization\")\n","    input_text_tokens = INPUT_TEXT.encode(\"utf-8\") # utf-8 encoded byte array\n","    input_text_tokens = list(map(int, input_text_tokens)) # convert all bytes into int\n","    original_tokens_len = len(input_text_tokens)\n","\n","    # develop the merge forest from the input data\n","    num_merges = BPE_VOCAB_SIZE - 256\n","    merges = {} # (int, int) -> int\n","    for i in range(num_merges):\n","        all_byte_pair = get_byte_pair_stats(tokens=input_text_tokens)\n","        top_byte_pair = max(all_byte_pair, key=all_byte_pair.get)\n","        new_token = 256 + i\n","        print(f\"merging {top_byte_pair} into a new token {new_token}\")\n","        input_text_tokens = merge_byte_pair(tokens=input_text_tokens, pair=top_byte_pair, new_token=new_token)\n","        merges[top_byte_pair] = new_token\n","\n","    print(f\"original token length {original_tokens_len}; now token length {len(input_text_tokens)}; compression %: {original_tokens_len / len(input_text_tokens):.2f}X\")\n","\n","    # calculate new vocab set\n","    vocab = {idx: bytes([idx]) for idx in range(256)}\n","    for (p0, p1), idx in merges.items():\n","        vocab[idx] = vocab[p0] + vocab[p1]\n","\n","    def bpe_decode(input_tokens):\n","        unmerged_tokens = b\"\".join(vocab[i] for i in input_tokens)\n","        return unmerged_tokens.decode(\"utf-8\", errors=\"replace\")\n","\n","    def bpe_encode(text):\n","        tokens = list(text.encode(\"utf-8\"))\n","        while len(tokens) >= 2:\n","            all_byte_pairs = get_byte_pair_stats(tokens=tokens)\n","            pair = min(all_byte_pairs, key=lambda p: merges.get(p, float(\"inf\")))\n","            if pair not in merges:\n","                break # the pair is not from the merge forest\n","            new_token = merges[pair]\n","            tokens = merge_byte_pair(tokens=tokens, pair=pair, new_token=new_token)\n","        return tokens\n","\n","    all_data = torch.tensor(input_text_tokens, dtype=torch.long)\n","\n","    return all_data, bpe_decode, bpe_encode\n","\n","\n","\n","def tokenize(tokenization_alg):\n","    \"\"\"\n","    input: type, one of \"default\", \"tiny-llama-fast-tokenizer\"\n","    output (training_data, validation_data, decode, encode, vocab_size)\n","      decode, encode are functions\n","    \"\"\"\n","\n","    if tokenization_alg == 'default':\n","        # each token is one character\n","        all_distinct_characters = sorted(list(set(INPUT_TEXT)))\n","        vocab_size = len(all_distinct_characters)\n","        stoi = {ch: i for i, ch in enumerate(all_distinct_characters)}\n","        itos = {i: ch for i, ch in enumerate(all_distinct_characters)}\n","        # encoder: take a string, output a list of integers\n","        def encode(s): return [stoi[c] for c in s]\n","        # decoder: take a list of integers, output a string\n","        def decode(l): return ''.join([itos[i] for i in l])\n","\n","        # encode text to long\n","        all_data = torch.tensor(encode(INPUT_TEXT), dtype=torch.long)\n","\n","    elif tokenization_alg == 'bpe':\n","        all_data, decode, encode = bpe()\n","        vocab_size = BPE_VOCAB_SIZE\n","\n","    elif tokenization_alg == 'tiny-llama-fast-tokenizer':\n","        vocab_size = 32000\n","        tokenizer = AutoTokenizer.from_pretrained(\n","            'fxmarty/tiny-llama-fast-tokenizer')\n","        all_data = torch.tensor(tokenizer(INPUT_TEXT).input_ids, dtype=torch.long)\n","        decode = tokenizer.decode\n","        encode = tokenizer.encode\n","\n","    train_data_size = int(TRAINING_DATA_SPLIT * len(all_data))\n","    train_data = all_data[:train_data_size]\n","    validation_data = all_data[train_data_size:]\n","\n","    print(f'{vocab_size=}')\n","    return train_data, validation_data, decode, encode, vocab_size\n","\n","\n","train_data, validation_data, decode, encode, vocab_size = tokenize(TOKENIZATION_ALG)"]},{"cell_type":"code","execution_count":8,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":105,"status":"ok","timestamp":1710563914480,"user":{"displayName":"He Nan Li (Tony)","userId":"01291724548798117557"},"user_tz":420},"id":"Iem1N619jPSf","outputId":"dba7db95-6c2d-4338-a525-6f0716d39185"},"outputs":[{"output_type":"stream","name":"stdout","text":["TORCH_MANUAL_SEED=1337\n","DEVICE='cuda'\n","BATCH_SIZE=32 BLOCK_SIZE=128\n","MAX_TRAINING_ITERATIONS=5000 LEARNING_RATE=0.0003\n","EVAL_INTERVAL=1000 EVAL_ITERS=20\n","N_EMBED=100 N_HEAD=5 N_LAYER=6\n","DROP_OUT=0.2\n"]}],"source":["# @title initialize model hyper parameter\n","\n","TORCH_MANUAL_SEED = 1337  # @param {type:\"number\"}\n","print(f'{TORCH_MANUAL_SEED=}')\n","torch.manual_seed(TORCH_MANUAL_SEED)\n","\n","DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n","print(f'{DEVICE=}')\n","\n","# @markdown how many independent sequences will we process in parallel\n","BATCH_SIZE = 32  # @param {type:\"number\"}\n","# @markdown what's the maximum context length for prediction\n","BLOCK_SIZE = 128  # @param {type:\"number\"}\n","print(f'{BATCH_SIZE=} {BLOCK_SIZE=}')\n","\n","MAX_TRAINING_ITERATIONS = 5000  # @param {type:\"number\"}\n","LEARNING_RATE = 3e-4  # @param {type:\"number\"}\n","print(f'{MAX_TRAINING_ITERATIONS=} {LEARNING_RATE=}')\n","\n","# @markdown for every EVAL_INTERVAL training iterations, we run evaluation\n","EVAL_INTERVAL = 1000  # @param {type:\"number\"}\n","# @markdown for each evaluation we run # EVAL_ITERS of iterations\n","EVAL_ITERS = 20  # @param {type:\"number\"}\n","print(f'{EVAL_INTERVAL=} {EVAL_ITERS=}')\n","\n","# @markdown number of embedding dimension\n","N_EMBED = 100  # @param {type:\"number\"}\n","# @markdown number of heads in multihead; N_EMBED needs to be divisible by N_HEAD\n","N_HEAD = 5  # @param {type:\"number\"}\n","# @markdown number of layers to run sequentially\n","N_LAYER = 6  # @param {type:\"number\"}\n","print(f'{N_EMBED=} {N_HEAD=} {N_LAYER=}')\n","\n","# @markdown drop out rate for regularization\n","DROP_OUT = 0.2  # @param {type:\"number\"}\n","print(f'{DROP_OUT=}')"]},{"cell_type":"code","execution_count":9,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":228542,"status":"ok","timestamp":1710564146681,"user":{"displayName":"He Nan Li (Tony)","userId":"01291724548798117557"},"user_tz":420},"id":"I1cGXEkdzVZf","outputId":"e81fd6af-72ef-4400-b6b7-cfda3564c4c7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model has 0.794476 M param\n","\n","Generate 100 tokens from untrained model:\n","\n","�<ܙƓ��2��\u001f�\u0016�ˬ�H�䰪\u0001\u0018Eo �A�}M�ۻ�]:�\u0003I��=������C<�\u001f���#s g Pll�[\u000f��\n","\n","\n","Begin training for 5000 iterations\n"]},{"output_type":"stream","name":"stderr","text":["  0%|          | 1/5000 [00:00<55:13,  1.51it/s]"]},{"output_type":"stream","name":"stdout","text":["Evaluation: train loss 5.8524, val loss 5.8492\n"]},{"output_type":"stream","name":"stderr","text":[" 20%|██        | 1004/5000 [00:44<05:04, 13.14it/s]"]},{"output_type":"stream","name":"stdout","text":["Evaluation: train loss 2.6707, val loss 2.7224\n"]},{"output_type":"stream","name":"stderr","text":[" 40%|████      | 2003/5000 [01:29<04:22, 11.43it/s]"]},{"output_type":"stream","name":"stdout","text":["Evaluation: train loss 2.3912, val loss 2.5053\n"]},{"output_type":"stream","name":"stderr","text":[" 60%|██████    | 3004/5000 [02:15<02:27, 13.49it/s]"]},{"output_type":"stream","name":"stdout","text":["Evaluation: train loss 2.2036, val loss 2.3757\n"]},{"output_type":"stream","name":"stderr","text":[" 80%|████████  | 4003/5000 [03:00<01:30, 10.97it/s]"]},{"output_type":"stream","name":"stdout","text":["Evaluation: train loss 2.0993, val loss 2.3015\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 5000/5000 [03:44<00:00, 22.28it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Training Completed\n","Loss after training 2.1685941219329834\n","Generate 100 tokens from trained model with empty context\n","\u0000urse\n","As sucuemen me jothersed and Anoidght;\n","From for swerd with weed. Warwif worls,\n","To be with out it while the cromed tend a\n"]}],"source":["# @title Transformer!\n","\n","# TODO: rewrite get_batch function so it's not random\n","# we should run epoch\n","def get_batch(split):\n","    \"\"\"\n","    generate a small batch (of BATCH_SIZE) of data of input x and target y\n","\n","    input:\n","        split: either \"train\" or \"eval\"\n","    output:\n","        (x, y): both x and y are of the same shape (BATCH_SIZE, BLOCK_SIZE);\n","        y is 1 right shift from x\n","    \"\"\"\n","    data = train_data if split == 'train' else validation_data\n","\n","    # len(data) - BLOCK_SIZE is the maximum random int\n","    # - BLOCK_SIZE because the last BLOCK_SIZE, the response variable y will go out of boundary\n","    # (BATCH_SIZE, ) is one dimensional array of size BATCH_SIZE\n","    random_idx = torch.randint(len(data) - BLOCK_SIZE, (BATCH_SIZE, ))\n","\n","    # x and y are (BATCH_SIZE, BLOCK_SIZE) shape\n","    # y is right shift by 1 from x\n","    x = torch.stack([data[i:i+BLOCK_SIZE] for i in random_idx])\n","    y = torch.stack([data[i+1:i+BLOCK_SIZE+1] for i in random_idx])\n","    x, y = x.to(DEVICE), y.to(DEVICE)\n","    return x, y\n","\n","\n","@torch.no_grad\n","def evaluate_loss():\n","    out = {}\n","    model.eval()  # setting to evaluation state\n","    for split in ['train', 'eval']:\n","        losses = torch.zeros(EVAL_ITERS)\n","        # for each EVAL_INTERVAL steps in training, we EVAL_ITERS steps to estimate loss\n","        for k in range(EVAL_ITERS):\n","            X, Y = get_batch(split)\n","            logits, loss = model(X, Y)\n","            losses[k] = loss.item()\n","        out[split] = losses.mean()\n","    model.train()  # setting to training state\n","    return out\n","\n","\n","class Head(nn.Module):\n","    \"\"\"one head of self-attention\"\"\"\n","\n","    def __init__(self, head_size):\n","        super().__init__()\n","        self.key = nn.Linear(N_EMBED, head_size, bias=False)\n","        self.query = nn.Linear(N_EMBED, head_size, bias=False)\n","        self.value = nn.Linear(N_EMBED, head_size, bias=False)\n","        self.register_buffer('tril', torch.tril(\n","            torch.ones(BLOCK_SIZE, BLOCK_SIZE)))\n","        self.dropout = nn.Dropout(DROP_OUT)\n","\n","    def forward(self, x):\n","        B, T, C = x.shape\n","        k = self.key(x)  # key (B, T, C)\n","        q = self.query(x)  # query\n","        # (B T C) @ (B C T) -> (B T T); divide by sqrt(d_k) to preserve variance\n","        weight = q @ k.transpose(-2, -1) * C**-0.5\n","        weight = weight.masked_fill(self.tril[:T, :T] == 0, float(\n","            '-inf'))  # mark all up right triangle and -inf\n","        weight = F.softmax(weight, dim=-1)  # B T T\n","        weight = self.dropout(weight)\n","        # weighted aggregation of the values\n","        value = self.value(x)\n","        return weight @ value\n","\n","\n","class MultiHead(nn.Module):\n","    \"\"\"multiple heads of self attention in parallel\"\"\"\n","\n","    def __init__(self, num_heads, head_size):\n","        super().__init__()\n","        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n","        self.proj = nn.Linear(N_EMBED, N_EMBED)\n","        self.dropout = nn.Dropout(DROP_OUT)\n","\n","    def forward(self, x):\n","        # concat over the channel dimension\n","        out = torch.cat([h(x) for h in self.heads], dim=-1)\n","        out = self.dropout(self.proj(out))\n","        return out\n","\n","\n","class FeedForward(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            # from pager section 3.3, d_ff = d_model * 4\n","            nn.Linear(N_EMBED, 4 * N_EMBED),\n","            nn.ReLU(),\n","            nn.Linear(4 * N_EMBED, N_EMBED),  # residual connection\n","            nn.Dropout(DROP_OUT),\n","        )\n","\n","    def forward(self, x):\n","        return self.net(x)\n","\n","\n","class Block(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        head_size = N_EMBED // N_HEAD\n","        self.sa = MultiHead(N_HEAD, head_size)\n","        self.ffwd = FeedForward()\n","        self.ln1 = nn.LayerNorm(N_EMBED)  # layer norm\n","        self.ln2 = nn.LayerNorm(N_EMBED)\n","\n","    def forward(self, x):\n","        # x = x + self.sa(self.ln1(x))  # residual connection\n","        x = self.ln1(x + self.sa(x))\n","        # x = x + self.ffwd(self.ln2(x))\n","        x = self.ln2(x + self.ffwd(x))\n","        return x\n","\n","\n","class BigramLM(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        # each token directly reads off the logits for the next token from a lookup table\n","        self.token_embedding_table = nn.Embedding(\n","            num_embeddings=vocab_size, embedding_dim=N_EMBED)\n","        self.position_embedding_table = nn.Embedding(\n","            num_embeddings=BLOCK_SIZE, embedding_dim=N_EMBED)\n","        # language model head\n","        # a linear transformation\n","        self.blocks = nn.Sequential(\n","            *[Block() for _ in range(N_LAYER)]\n","        )\n","        self.ln_f = nn.LayerNorm(N_EMBED)  # final layer norm\n","        self.lm_head = nn.Linear(in_features=N_EMBED, out_features=vocab_size)\n","\n","    def forward(self, x_batch, target=None):\n","        \"\"\"\n","        input:\n","            x_batch, target: (BATCH_SIZE, time) shape tensor\n","                BATCH_SIZE = 4; time = 8 = BLOCK_SIZE\n","        return:\n","            logits: shape (B * T, C)\n","            loss: optional float\n","        \"\"\"\n","\n","        B, T = x_batch.shape\n","\n","        # logits of shape (BATCH_SIZE, time, channel)\n","        # channel is vocab size\n","        # for each batch, we predict for each index, the logits to predict the next char\n","        # among all vocab size\n","        token_embed = self.token_embedding_table(x_batch)  # (B, T, n_embed)\n","        position_embed = self.position_embedding_table(\n","            torch.arange(T, device=DEVICE))  # shape (T, C)\n","        x = token_embed + position_embed\n","        x = self.blocks(x)\n","        x = self.ln_f(x)\n","        logits = self.lm_head(x)  # (B, T, vocab_size)\n","\n","        # loss function = negative log likelihood (cross entropy against targets)\n","        # need to reshape logits and y_batch for the cross_entropy function\n","        if target is None:\n","            return logits, None\n","        else:\n","            B, T, C = logits.shape\n","            logits = logits.view(B*T, C)\n","            target = target.view(B*T)\n","            loss = F.cross_entropy(logits, target=target)\n","            return logits, loss\n","\n","    def generate(self, x_batch, max_new_tokens):\n","        \"\"\"\n","        generate the next max_new_tokens from x_batch, being the current context\n","\n","        input:\n","            x_batch: (B, T) array of indices\n","        \"\"\"\n","        # TODO: we should update the x batch with the newly generated token\n","        for _ in range(max_new_tokens):\n","            # crop out the last block size\n","            x_batch_bounded = x_batch[:, -BLOCK_SIZE:]\n","            logits, loss = self(x_batch_bounded)\n","            # becomes (B, C), get the last column in T\n","            logits = logits[:, -1, :]\n","            probs = F.softmax(logits, dim=-1)\n","            predicted_next_index = torch.multinomial(\n","                probs, num_samples=1)  # (B, 1)\n","            x_batch = torch.cat(\n","                (x_batch, predicted_next_index), dim=1)  # (B, T+1)\n","        return x_batch\n","\n","\n","model = BigramLM()\n","m = model.to(device=DEVICE)\n","print('Model has', sum(p.numel() for p in m.parameters()) / 1e6, \"M param\")\n","\n","# predict 100 tokens from token value 0\n","print(\"\\nGenerate 100 tokens from untrained model:\\n\")\n","print(decode(m.generate(x_batch=torch.zeros(\n","    (1, 1), dtype=torch.long, device=DEVICE), max_new_tokens=100)[0].tolist()))\n","print('\\n')\n","\n","\n","# create pytorch optimizer\n","optimizer = torch.optim.AdamW(m.parameters(), lr=LEARNING_RATE)\n","\n","# train for these many iterations\n","print(f\"Begin training for {MAX_TRAINING_ITERATIONS} iterations\")\n","for step in tqdm(range(MAX_TRAINING_ITERATIONS)):\n","    if step % EVAL_INTERVAL == 0:\n","        losses = evaluate_loss()\n","        print(\n","            f\"Evaluation: train loss {losses['train']:.4f}, val loss {losses['eval']:.4f}\")\n","\n","    x_batch, y_batch = get_batch('train')\n","    logits, loss = m(x_batch, y_batch)\n","    optimizer.zero_grad(set_to_none=True)\n","    loss.backward()\n","    optimizer.step()\n","\n","print(f'Training Completed')\n","print(f\"Loss after training {loss.item()}\")\n","\n","# re try generate tokens after training\n","print(\"Generate 100 tokens from trained model with empty context\")\n","empty_context = torch.zeros((1, 1), dtype=torch.long, device=DEVICE)\n","print(decode(m.generate(x_batch=empty_context,\n","      max_new_tokens=100)[0].tolist()))"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"id":"ZzGtMx6JrY_-","outputId":"7f50acb4-18f3-45be-f54f-5be0437467cd"},"outputs":[{"output_type":"stream","name":"stdout","text":["CONTEXT=' '\n","GENERATE_TOKEN_LIMIT=1000\n","\n","Generate from model:\n","\n"]}],"source":["CONTEXT = ' '  # @param {type:\"string\"}\n","print(f'{CONTEXT=}')\n","GENERATE_TOKEN_LIMIT = 1000  # @param {type:\"number\"}\n","print(f'{GENERATE_TOKEN_LIMIT=}')\n","\n","encoded_context = torch.tensor(\n","    [encode(CONTEXT)], dtype=torch.long, device=DEVICE)\n","\n","print('\\nGenerate from model:\\n')\n","print(decode(m.generate(x_batch=encoded_context,\n","      max_new_tokens=GENERATE_TOKEN_LIMIT)[0].tolist()))"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"}},"nbformat":4,"nbformat_minor":0}