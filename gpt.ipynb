{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19137,"status":"ok","timestamp":1710138609828,"user":{"displayName":"He Nan Li (Tony)","userId":"01291724548798117557"},"user_tz":420},"id":"JSZ5i4yG7-lp","outputId":"2512b58c-389d-46ca-8f7f-9e9393132e6f"},"outputs":[],"source":["# @title init for running on colab\n","!pip install datasets\n","!pip install tqdm\n","\n","import torch\n","import torch.nn as nn\n","from torch.nn import functional as F\n","from transformers import AutoTokenizer\n","from datasets import load_dataset\n","from tqdm import tqdm\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":119,"status":"ok","timestamp":1710140031395,"user":{"displayName":"He Nan Li (Tony)","userId":"01291724548798117557"},"user_tz":420},"id":"CCarHxmF6Kxt","outputId":"6cef8f73-e092-4d01-8573-6de0f1a6a3e8"},"outputs":[],"source":["# @title load data source\n","\n","\n","DATASET_SOURCE = 'local_file' # @param [\"wikimedia/wikipedia/20231101.zh-classical\", \"local_file\"]\n","\n","LOCAL_FILE = 'input2.txt' # @param {type:\"string\"}\n","\n","print(f'{DATASET_SOURCE=}')\n","\n","def load_dataset_from_source():\n","    if DATASET_SOURCE == 'local_file':\n","        with open(LOCAL_FILE, 'r', encoding='utf-8') as f:\n","            text = f.read()\n","        print('final input text length', len(text))\n","        return text\n","\n","    text = ''\n","    if DATASET_SOURCE == 'wikimedia/wikipedia/20231101.zh-classical':\n","        dataset = load_dataset('wikimedia/wikipedia', '20231101.zh-classical')\n","        num_rows = dataset['train'].num_rows\n","\n","    for i in range(num_rows):\n","        text += dataset['train'][i]['text']\n","    print('final input text length', len(text))\n","    return text\n","\n","\n","INPUT_TEXT = load_dataset_from_source()"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":148,"status":"ok","timestamp":1710140035065,"user":{"displayName":"He Nan Li (Tony)","userId":"01291724548798117557"},"user_tz":420},"id":"b7khzRaHhx0p","outputId":"0c4ca6f0-f914-466f-d51b-05f9e50b9a58"},"outputs":[],"source":["# @title select tokenization alg\n","\n","TOKENIZATION_ALG = 'bpe'  # @param [\"default\", \"tiny-llama-fast-tokenizer\", \"bpe\"]\n","TRAINING_DATA_SPLIT = 0.9  # @param {type:\"slider\", min:0, max:1, step:0.05}\n","\n","# @markdown The following hyper parameters are only applicable for BPE\n","BPE_VOCAB_SIZE = 276 # @param {type:\"number\"}\n","\n","print(f'{TOKENIZATION_ALG=} {TRAINING_DATA_SPLIT=}')\n","\n","\n","def bpe():\n","\n","    def get_byte_pair_stats(tokens):\n","        \"\"\"\n","        Return one byte pair with the highest frequency\n","        \"\"\"\n","        counts = {}\n","        for pair in zip(tokens, tokens[1:]): # iterate through consecutive elements\n","            counts[pair] = counts.get(pair, 0) + 1\n","        return counts\n","\n","\n","    def merge_byte_pair(tokens, pair, new_token):\n","        \"\"\"\n","        tokens: list of token bytes\n","        pair: byte pair to be replaced\n","        new_token: new byte\n","\n","        return a new list of token bytes after replacement\n","\n","        example: merge_byte_pair([5,6,6,7,9], (6,7), 99) -> [5, 6, 99, 9]\n","        \"\"\"\n","        i = 0\n","        new_tokens = []\n","        while i < len(tokens):\n","            if i < len(tokens) - 1 and tokens[i] == pair[0] and tokens[i+1] == pair[1]:\n","                new_tokens.append(new_token)\n","                i += 2\n","            else:\n","                new_tokens.append(tokens[i])\n","                i += 1\n","        return new_tokens\n","\n","    print(\"begin byte pair tokenization\")\n","    input_text_tokens = INPUT_TEXT.encode(\"utf-8\") # utf-8 encoded byte array\n","    input_text_tokens = list(map(int, input_text_tokens)) # convert all bytes into int\n","    original_tokens_len = len(input_text_tokens)\n","\n","    # develop the merge forest from the input data\n","    num_merges = BPE_VOCAB_SIZE - 256\n","    merges = {} # (int, int) -> int\n","    for i in range(num_merges):\n","        all_byte_pair = get_byte_pair_stats(tokens=input_text_tokens)\n","        top_byte_pair = max(all_byte_pair, key=all_byte_pair.get)\n","        new_token = 256 + i\n","        print(f\"merging {top_byte_pair} into a new token {new_token}\")\n","        input_text_tokens = merge_byte_pair(tokens=input_text_tokens, pair=top_byte_pair, new_token=new_token)\n","        merges[top_byte_pair] = new_token\n","\n","    print(f\"original token length {original_tokens_len}; now token length {len(input_text_tokens)}; compression %: {original_tokens_len / len(input_text_tokens):.2f}X\")\n","\n","    # calculate new vocab set\n","    vocab = {idx: bytes([idx]) for idx in range(256)}\n","    for (p0, p1), idx in merges.items():\n","        vocab[idx] = vocab[p0] + vocab[p1]\n","\n","    def bpe_decode(input_tokens):\n","        unmerged_tokens = b\"\".join(vocab[i] for i in input_tokens)\n","        return unmerged_tokens.decode(\"utf-8\", errors=\"replace\")\n","\n","    def bpe_encode(text):\n","        tokens = list(text.encode(\"utf-8\"))\n","        while len(tokens) >= 2:\n","            all_byte_pairs = get_byte_pair_stats(tokens=tokens)\n","            pair = min(all_byte_pairs, key=lambda p: merges.get(p, float(\"inf\")))\n","            if pair not in merges:\n","                break # the pair is not from the merge forest\n","            new_token = merges[pair]\n","            tokens = merge_byte_pair(tokens=tokens, pair=pair, new_token=new_token)\n","        return tokens\n","\n","    all_data = torch.tensor(input_text_tokens, dtype=torch.long)\n","\n","    return all_data, bpe_decode, bpe_encode\n","\n","\n","\n","def tokenize(tokenization_alg):\n","    \"\"\"\n","    input: type, one of \"default\", \"tiny-llama-fast-tokenizer\"\n","    output (training_data, validation_data, decode, encode, vocab_size)\n","      decode, encode are functions\n","    \"\"\"\n","\n","    if tokenization_alg == 'default':\n","        # each token is one character\n","        all_distinct_characters = sorted(list(set(INPUT_TEXT)))\n","        vocab_size = len(all_distinct_characters)\n","        stoi = {ch: i for i, ch in enumerate(all_distinct_characters)}\n","        itos = {i: ch for i, ch in enumerate(all_distinct_characters)}\n","        # encoder: take a string, output a list of integers\n","        def encode(s): return [stoi[c] for c in s]\n","        # decoder: take a list of integers, output a string\n","        def decode(l): return ''.join([itos[i] for i in l])\n","\n","        # encode text to long\n","        all_data = torch.tensor(encode(INPUT_TEXT), dtype=torch.long)\n","\n","    elif tokenization_alg == 'bpe':\n","        all_data, decode, encode = bpe()\n","        vocab_size = BPE_VOCAB_SIZE\n","\n","    elif tokenization_alg == 'tiny-llama-fast-tokenizer':\n","        vocab_size = 32000\n","        tokenizer = AutoTokenizer.from_pretrained(\n","            'fxmarty/tiny-llama-fast-tokenizer')\n","        all_data = torch.tensor(tokenizer(INPUT_TEXT).input_ids, dtype=torch.long)\n","        decode = tokenizer.decode\n","        encode = tokenizer.encode\n","\n","    train_data_size = int(TRAINING_DATA_SPLIT * len(all_data))\n","    train_data = all_data[:train_data_size]\n","    validation_data = all_data[train_data_size:]\n","\n","    print(f'{vocab_size=}')\n","    return train_data, validation_data, decode, encode, vocab_size\n","\n","\n","train_data, validation_data, decode, encode, vocab_size = tokenize(TOKENIZATION_ALG)"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":118,"status":"ok","timestamp":1710140046346,"user":{"displayName":"He Nan Li (Tony)","userId":"01291724548798117557"},"user_tz":420},"id":"Iem1N619jPSf","outputId":"b8875986-14e0-4dd8-a56e-30cb1c1a06df"},"outputs":[],"source":["# @title initialize model hyper parameter\n","\n","TORCH_MANUAL_SEED = 1337  # @param {type:\"number\"}\n","print(f'{TORCH_MANUAL_SEED=}')\n","torch.manual_seed(TORCH_MANUAL_SEED)\n","\n","DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n","print(f'{DEVICE=}')\n","\n","# @markdown how many independent sequences will we process in parallel\n","BATCH_SIZE = 64  # @param {type:\"number\"}\n","# @markdown what's the maximum context length for prediction\n","BLOCK_SIZE = 128  # @param {type:\"number\"}\n","print(f'{BATCH_SIZE=} {BLOCK_SIZE=}')\n","\n","MAX_TRAINING_ITERATIONS = 1000  # @param {type:\"number\"}\n","LEARNING_RATE = 1e-3  # @param {type:\"number\"}\n","print(f'{MAX_TRAINING_ITERATIONS=} {LEARNING_RATE=}')\n","\n","# @markdown for every EVAL_INTERVAL training iterations, we run evaluation\n","EVAL_INTERVAL = 300  # @param {type:\"number\"}\n","# @markdown for each evaluation we run # EVAL_ITERS of iterations\n","EVAL_ITERS = 20  # @param {type:\"number\"}\n","print(f'{EVAL_INTERVAL=} {EVAL_ITERS=}')\n","\n","# @markdown number of embedding dimension\n","N_EMBED = 100  # @param {type:\"number\"}\n","# @markdown number of heads in multihead; N_EMBED needs to be divisible by N_HEAD\n","N_HEAD = 5  # @param {type:\"number\"}\n","# @markdown number of layers to run sequentially\n","N_LAYER = 6  # @param {type:\"number\"}\n","print(f'{N_EMBED=} {N_HEAD=} {N_LAYER=}')\n","\n","# @markdown drop out rate for regularization\n","DROP_OUT = 0.2  # @param {type:\"number\"}\n","print(f'{DROP_OUT=}')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":58112,"status":"ok","timestamp":1710140106984,"user":{"displayName":"He Nan Li (Tony)","userId":"01291724548798117557"},"user_tz":420},"id":"I1cGXEkdzVZf","outputId":"4573e35d-4200-48a2-d11b-10bb2841faeb"},"outputs":[],"source":["# @title Transformer!\n","\n","# TODO: rewrite get_batch function so it's not random\n","# we should run epoch\n","def get_batch(split):\n","    \"\"\"\n","    generate a small batch (of BATCH_SIZE) of data of input x and target y\n","\n","    input:\n","        split: either \"train\" or \"eval\"\n","    output:\n","        (x, y): both x and y are of the same shape (BATCH_SIZE, BLOCK_SIZE);\n","        y is 1 right shift from x\n","    \"\"\"\n","    data = train_data if split == 'train' else validation_data\n","\n","    # len(data) - BLOCK_SIZE is the maximum random int\n","    # - BLOCK_SIZE because the last BLOCK_SIZE, the response variable y will go out of boundary\n","    # (BATCH_SIZE, ) is one dimensional array of size BATCH_SIZE\n","    random_idx = torch.randint(len(data) - BLOCK_SIZE, (BATCH_SIZE, ))\n","\n","    # x and y are (BATCH_SIZE, BLOCK_SIZE) shape\n","    # y is right shift by 1 from x\n","    x = torch.stack([data[i:i+BLOCK_SIZE] for i in random_idx])\n","    y = torch.stack([data[i+1:i+BLOCK_SIZE+1] for i in random_idx])\n","    x, y = x.to(DEVICE), y.to(DEVICE)\n","    return x, y\n","\n","\n","@torch.no_grad\n","def evaluate_loss():\n","    out = {}\n","    model.eval()  # setting to evaluation state\n","    for split in ['train', 'eval']:\n","        losses = torch.zeros(EVAL_ITERS)\n","        # for each EVAL_INTERVAL steps in training, we EVAL_ITERS steps to estimate loss\n","        for k in range(EVAL_ITERS):\n","            X, Y = get_batch(split)\n","            logits, loss = model(X, Y)\n","            losses[k] = loss.item()\n","        out[split] = losses.mean()\n","    model.train()  # setting to training state\n","    return out\n","\n","\n","class Head(nn.Module):\n","    \"\"\"one head of self-attention\"\"\"\n","\n","    def __init__(self, head_size):\n","        super().__init__()\n","        self.key = nn.Linear(N_EMBED, head_size, bias=False)\n","        self.query = nn.Linear(N_EMBED, head_size, bias=False)\n","        self.value = nn.Linear(N_EMBED, head_size, bias=False)\n","        self.register_buffer('tril', torch.tril(\n","            torch.ones(BLOCK_SIZE, BLOCK_SIZE)))\n","        self.dropout = nn.Dropout(DROP_OUT)\n","\n","    def forward(self, x):\n","        B, T, C = x.shape\n","        k = self.key(x)  # key (B, T, C)\n","        q = self.query(x)  # query\n","        # (B T C) @ (B C T) -> (B T T); divide by sqrt(d_k) to preserve variance\n","        weight = q @ k.transpose(-2, -1) * C**-0.5\n","        weight = weight.masked_fill(self.tril[:T, :T] == 0, float(\n","            '-inf'))  # mark all up right triangle and -inf\n","        weight = F.softmax(weight, dim=-1)  # B T T\n","        weight = self.dropout(weight)\n","        # weighted aggregation of the values\n","        value = self.value(x)\n","        return weight @ value\n","\n","\n","class MultiHead(nn.Module):\n","    \"\"\"multiple heads of self attention in parallel\"\"\"\n","\n","    def __init__(self, num_heads, head_size):\n","        super().__init__()\n","        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n","        self.proj = nn.Linear(N_EMBED, N_EMBED)\n","        self.dropout = nn.Dropout(DROP_OUT)\n","\n","    def forward(self, x):\n","        # concat over the channel dimension\n","        out = torch.cat([h(x) for h in self.heads], dim=-1)\n","        out = self.dropout(self.proj(out))\n","        return out\n","\n","\n","class FeedForward(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            # from pager section 3.3, d_ff = d_model * 4\n","            nn.Linear(N_EMBED, 4 * N_EMBED),\n","            nn.ReLU(),\n","            nn.Linear(4 * N_EMBED, N_EMBED),  # residual connection\n","            nn.Dropout(DROP_OUT),\n","        )\n","\n","    def forward(self, x):\n","        return self.net(x)\n","\n","\n","class Block(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        head_size = N_EMBED // N_HEAD\n","        self.sa = MultiHead(N_HEAD, head_size)\n","        self.ffwd = FeedForward()\n","        self.ln1 = nn.LayerNorm(N_EMBED)  # layer norm\n","        self.ln2 = nn.LayerNorm(N_EMBED)\n","\n","    def forward(self, x):\n","        # x = x + self.sa(self.ln1(x))  # residual connection\n","        x = self.ln1(x + self.sa(x))\n","        # x = x + self.ffwd(self.ln2(x))\n","        x = self.ln2(x + self.ffwd(x))\n","        return x\n","\n","\n","class BigramLM(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        # each token directly reads off the logits for the next token from a lookup table\n","        self.token_embedding_table = nn.Embedding(\n","            num_embeddings=vocab_size, embedding_dim=N_EMBED)\n","        self.position_embedding_table = nn.Embedding(\n","            num_embeddings=BLOCK_SIZE, embedding_dim=N_EMBED)\n","        # language model head\n","        # a linear transformation\n","        self.blocks = nn.Sequential(\n","            *[Block() for _ in range(N_LAYER)]\n","        )\n","        self.ln_f = nn.LayerNorm(N_EMBED)  # final layer norm\n","        self.lm_head = nn.Linear(in_features=N_EMBED, out_features=vocab_size)\n","\n","    def forward(self, x_batch, target=None):\n","        \"\"\"\n","        input:\n","            x_batch, target: (BATCH_SIZE, time) shape tensor\n","                BATCH_SIZE = 4; time = 8 = BLOCK_SIZE\n","        return:\n","            logits: shape (B * T, C)\n","            loss: optional float\n","        \"\"\"\n","\n","        B, T = x_batch.shape\n","\n","        # logits of shape (BATCH_SIZE, time, channel)\n","        # channel is vocab size\n","        # for each batch, we predict for each index, the logits to predict the next char\n","        # among all vocab size\n","        token_embed = self.token_embedding_table(x_batch)  # (B, T, n_embed)\n","        position_embed = self.position_embedding_table(\n","            torch.arange(T, device=DEVICE))  # shape (T, C)\n","        x = token_embed + position_embed\n","        x = self.blocks(x)\n","        x = self.ln_f(x)\n","        logits = self.lm_head(x)  # (B, T, vocab_size)\n","\n","        # loss function = negative log likelihood (cross entropy against targets)\n","        # need to reshape logits and y_batch for the cross_entropy function\n","        if target is None:\n","            return logits, None\n","        else:\n","            B, T, C = logits.shape\n","            logits = logits.view(B*T, C)\n","            target = target.view(B*T)\n","            loss = F.cross_entropy(logits, target=target)\n","            return logits, loss\n","\n","    def generate(self, x_batch, max_new_tokens):\n","        \"\"\"\n","        generate the next max_new_tokens from x_batch, being the current context\n","\n","        input:\n","            x_batch: (B, T) array of indices\n","        \"\"\"\n","        for _ in range(max_new_tokens):\n","            # crop out the last block size\n","            x_batch_bounded = x_batch[:, -BLOCK_SIZE:]\n","            logits, loss = self(x_batch_bounded)\n","            # becomes (B, C), get the last column in T\n","            logits = logits[:, -1, :]\n","            probs = F.softmax(logits, dim=-1)\n","            predicted_next_index = torch.multinomial(\n","                probs, num_samples=1)  # (B, 1)\n","            x_batch = torch.cat(\n","                (x_batch, predicted_next_index), dim=1)  # (B, T+1)\n","        return x_batch\n","\n","\n","model = BigramLM()\n","m = model.to(device=DEVICE)\n","print('Model has', sum(p.numel() for p in m.parameters()) / 1e6, \"M param\")\n","\n","# predict 100 tokens from token value 0\n","print(\"\\nGenerate 100 tokens from untrained model:\\n\")\n","print(decode(m.generate(x_batch=torch.zeros(\n","    (1, 1), dtype=torch.long, device=DEVICE), max_new_tokens=100)[0].tolist()))\n","print('\\n')\n","\n","\n","# create pytorch optimizer\n","optimizer = torch.optim.AdamW(m.parameters(), lr=LEARNING_RATE)\n","\n","# train for these many iterations\n","print(f\"Begin training for {MAX_TRAINING_ITERATIONS} iterations\")\n","for step in tqdm(range(MAX_TRAINING_ITERATIONS)):\n","    if step % EVAL_INTERVAL == 0:\n","        losses = evaluate_loss()\n","        print(\n","            f\"Evaluation: train loss {losses['train']:.4f}, val loss {losses['eval']:.4f}\")\n","\n","    x_batch, y_batch = get_batch('train')\n","    logits, loss = m(x_batch, y_batch)\n","    optimizer.zero_grad(set_to_none=True)\n","    loss.backward()\n","    optimizer.step()\n","\n","print(f'Training Completed')\n","print(f\"Loss after training {loss.item()}\")\n","\n","# re try generate tokens after training\n","print(\"Generate 100 tokens from trained model with empty context\")\n","empty_context = torch.zeros((1, 1), dtype=torch.long, device=DEVICE)\n","print(decode(m.generate(x_batch=empty_context,\n","      max_new_tokens=100)[0].tolist()))"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2804,"status":"ok","timestamp":1710140155217,"user":{"displayName":"He Nan Li (Tony)","userId":"01291724548798117557"},"user_tz":420},"id":"ZzGtMx6JrY_-","outputId":"f43261d9-1700-4801-ce11-f7361ef9cbbc"},"outputs":[],"source":["CONTEXT = '历史'  # @param {type:\"string\"}\n","print(f'{CONTEXT=}')\n","GENERATE_TOKEN_LIMIT = 100  # @param {type:\"number\"}\n","print(f'{GENERATE_TOKEN_LIMIT=}')\n","\n","encoded_context = torch.tensor(\n","    [encode(CONTEXT)], dtype=torch.long, device=DEVICE)\n","\n","print('\\nGenerate from model:\\n')\n","print(decode(m.generate(x_batch=encoded_context,\n","      max_new_tokens=GENERATE_TOKEN_LIMIT)[0].tolist()))"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"}},"nbformat":4,"nbformat_minor":0}
